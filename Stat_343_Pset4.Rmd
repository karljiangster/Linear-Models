---
title: "Stat 343 Pset 4"
author: "Karl Jiang"
date: "October 29, 2017"
output: pdf_document
---


```{r, include = FALSE}
library(alr4)
library(faraway)
library(knitr)
library(splines)
```

#1) Prostate 

##a) Forward Elimination 
```{r}
data("prostate")

get_sig_lowest_predictor = function(df, vars, alpha){
  cols = colnames(df)
  unused = cols[ !cols %in% vars ]
  
  best = ""
  lowest_p = 1
  
  for(i in 1:length( unused ) ){
    v = unused[i]
    
    preds = c(vars, v)
    mdl = lm(lpsa ~ . , data = df[, preds])
    s = summary(mdl)

    if(check_all_sig(s)){
      prob = s$coeff[v, 4]
      if(lowest_p > prob){
        best = v
        lowest_p = prob
      }    
    }
  }
  return(best)
}


print_added = function(s, v){
  print( sprintf("Variable Added: %s", v) )
  print( sprintf("P value: %f", s$coeff[v, 4] ) )
  print( sprintf("R^2 after adding: %f", s$r.squared) ) 
}

check_all_sig = function(s){
  nc = nrow(s$coeff)
  if(sum(s$coeff[2:nc, 4] > alpha)){
    #print(s$coeff[2:nc, 4])
    return(FALSE)  
  }
  return(TRUE)
}

forward_elim = function(df, y, alpha){
  vars = c(y) 
  go = TRUE 
  while( go ){
    var = get_sig_lowest_predictor(df, vars, alpha)
    #print(var)
    if(var != ""){
      vars = c(vars, var)
      s = summary(lm(lpsa ~ ., data = df[,vars]))
      print_added(s, var)
    }
    else{ go = FALSE}
  }
  return(vars)
}
```

```{r}
y = "lpsa"
vars = c(y)
alpha = 0.05

vars_forward = forward_elim(prostate, y, 0.05)
df_forward = prostate[, c(vars_forward)]
mdl_forward = lm(lpsa ~ ., data = df_forward)
s_forward = summary(mdl_forward)
print(s_forward)
```

##b) Backwards Elimnation

```{r}
backward_elimination = function(df, alpha){
  go = TRUE 
  vars = colnames(df)
  while(go){
    mdl = lm(lpsa ~ ., data = df[, vars])
    var = get_max_p_value(mdl, alpha)
    if(var != ""){
      vars = vars[!vars %in% var]
      print(vars)
    }
    else{
      go = FALSE 
    }
  }
  return(vars)
}

get_max_p_value = function(mdl, alpha){
  #returns predictor of corresponding to max p value that is > alpha  
  # o.w. returns empty string ""
  s = summary(mdl)
  
  p = nrow(s$coeff)
  var_rm = names( which.max( s$coeff[2:p, 4] ) ) 
  #print(var_rm)
  #print(s$coeff[var_rm, 4])
  if(s$coeff[var_rm, 4] < alpha){
    return("")
  }
  
  print_var_removed(s, var_rm)
  return(var_rm)
} 

print_var_removed = function(s, var_removed){
  print(sprintf("Variable Removed: %s", var_removed) ) 
  print( sprintf("P value: %f", s$coeff[var_removed, 4] ) )
  print( sprintf("R^2 before removal: %f", s$r.squared) ) 
}

```

### Test Backwards Elim
```{r}
y = "lpsa"
alpha = 0.05

vars_backward = backward_elimination(prostate, alpha)
print("Predictors after backward elim: ")
print(vars_backward)

df_backward = prostate[, c(vars_backward)]
mdl_backward = lm(lpsa ~ ., data = df_backward)
s_backward = summary(mdl_backward)
print(s_backward)
```

##c) AIC

$AIC = n log(RSS/n) + 2p$
There are a total of $2^8 - 1 = 255$ different models. In the function below, we return model with the lowest AIC by running all possible models and selecting the one with the lowest AIC.

```{r}
get_best_aic_model = function(df, k = 2){
  p = ncol(df) 
  vars = colnames(df)
  preds = vars[!vars %in% "lpsa"]
  
  null = lm(lpsa ~ 1, data = df)
  lowestAIC = extractAIC(null, k = k)[2]
  best = null
  count = 0 
  for(size in 1:(p-1)){
    combs = combn(preds, size)
      for(i in 1:ncol(combs)){
        tmp_vars = c( combs[, i], "lpsa" )
        count = count + 1
        mdl = lm(lpsa ~ ., data = df[,tmp_vars])
        mdl_aic = extractAIC(mdl, k = k)[2]
        if(lowestAIC > mdl_aic){
          lowestAIC = mdl_aic 
          best = mdl 
        }
      }
  }
  #print(count)
  return(best)
}
```

```{r}
mdl_aic = get_best_aic_model(prostate)
summary(mdl_aic)
extractAIC(mdl_aic)[2]
#mdl_aic_step = step(null, scope = formula(full), direction = "forward")
#extractAIC(mdl_aic_step)
```

##d) BIC 
$BIC = n log(RSS/n) + plog(n)$
```{r}
n = nrow(prostate)
mdl_bic = get_best_aic_model(prostate, k = log(n))
summary(mdl_bic)
extractAIC(mdl_bic)[2]
```

## Comparison

###a) vs b) Forward vs Backward  
Both the forward selection and the backward elimination give the same model. Typically, I would imagine that forward selection would give a more parsimonious model, which is more preferred. 

###b) vs c) Bacward vs AIC
The problem with backward elimination is that we might miss the "optimal" model. It's also hard to set a threshold for elimination (since we are performing tests many times). On the other hand, the exhuastive method of AIC ($2^d - 1$) will give us the "optimal" model (in AIC), but it comes at a computation cost. (Backward Selection is much cheaper) 
###c vs d), AIC vs BIC 
The primary difference in AIC and BIC is the penalization term on the model complexity. In the SLR case, for instance, AIC has a constant $2$ term multiplied by the dimension of the predictor space, and BIC has the nterm $log(n)$. In short, BIC penalizes the model complexity more --> simpler model. 

#2) Splines 

##a) Plot fit 

```{r}
n = 1000
x = seq(0, 1, length.out = n) 
f = function(x){ return( (sin(2 * pi * x))^3 ) } 
f_x = f(x)
y = f_x + rnorm(n = n, sd = 0.1)
plot(y ~ x)
lines(f_x ~ x, col = "red", lwd = 5) 

b_spline = bs(x, 12)
mdl_spline = lm(y ~ b_spline)
lines(mdl_spline$fitted.values ~ x, col = "blue", lwd = 3)
legend("topright", legend =  c("true = red", "spline fit = blue"))
```

##b) AIC 
```{r}
aic_12 = extractAIC(mdl_spline)[2]
print( sprintf( "AIC 12 splines: %f", aic_12) )
```

##c) AIC k = 3 -> 20
```{r}
aics = c() 
knot_sizes = 3:20
for(k in knot_sizes){
  b_spline_k = bs(x, k)
  mdl_spline_k = lm(y ~ b_spline_k)
  aics = c(aics, extractAIC(mdl_spline_k)[2])
}

plot(aics ~ knot_sizes, main = "AIC for different sized knots")
best_knot_size = knot_sizes[ which.min(aics) ] 
print(sprintf("The best knot size is %d knots", best_knot_size))
```

The best knot size is k = 16 evenly spaced knots.

##d) Fitting "best" knot

```{r}
plot(y ~ x, main = "Spline fit, knots = argmin AIC")
lines(f_x ~ x, col = "red", lwd = 5) 

b_spline_best = bs(x, best_knot_size)
mdl_spline_best = lm(y ~ b_spline_best)
lines(mdl_spline_best$fitted.values ~ x, col = "blue", lwd = 3)
legend("topright", legend =  c("true = red", "spline fit = blue"))
```

In comparision to part a) with 12 knots, the spline fit that minimizes AIC isn't noticably different. This is also evidenced in the AIC plot - the AIC between 12 and 16 knots is small. 

#3) Climate Data

##a) Linear Trend

```{r}
data("aatemp")
plot(temp ~ year, data = aatemp, main = "Temperature vs Year")
mdl_raw = lm(temp ~ year, data = aatemp)
abline(mdl_raw)
```

Because the data is so noisy, it's hard to say whether or not there is a linear trend. It looks like temperatures in general are rising up until 1950, but starts to sightly decline after that. 

##b) AR1 

"Observations in successive years may be correlated" suggests an AR(1) model. We note that there are entries where the most recent year may not be the previous year. To keep as much data as we can, we'll ignore data points where the lagged term is greater than 5 years. 
```{r}
thresh = 5 #anything with a lag year greater than this number will not be included 
d_over = which( diff(aatemp$year) >= thresh ) #bad indices 
tmp_lag = cbind( embed(aatemp$temp, 2), aatemp$year[2:nrow(aatemp)])
colnames( tmp_lag ) = c("temp", "lag", "year")
tmp_lag_clean = as.data.frame( tmp_lag[-c(d_over), ] ) 
```

Now let's fit the psuedo AR(1) model: 

```{r}
mdl_lag = lm(temp ~ lag, data = tmp_lag_clean)
summary(mdl_lag)
```

While the squared correlation between the lag and the current temp is somewhat low (.07719), the coefficient of the lag term under the $H_0: \beta_{lag} = 0$ is rejected by the significant p value. 

$\\$ 

Because the lag coefficient is also <1, it would look like the temperatures are not increasing, but rather a stationary process with a fixed expectation: $\frac{\beta_0}{1 - \beta_{lag}}$ 

##c) Backward Elim

Question is a little ambiguous with regards to how we should do backward elimination. Should be 1) eliminate the predictor with the highest insiginificant p value (which is how the algorithm is specified in Faraway), or 2) eliminate the predictor with the highest degree provided not all predictors are significant (for interpretibillity). Here, I'll use the former method. Also, note that I subtract min(years) because of integer overflow. 

```{r}
backward_elimination_temp = function(df, alpha){
  go = TRUE 
  vars = colnames(df)
  while(go){
    mdl = lm(temp ~ ., data = df[, vars])
    var = get_max_p_value(mdl, alpha)
    if(var != ""){
      vars = vars[!vars %in% var]
      print(vars)
    }
    else{
      go = FALSE 
    }
  }
  return(vars)
}
```

```{r}
tmp_poly = aatemp 
tmp_poly$year = aatemp$year - min(aatemp$year) 

polys = 2:10
for(i in polys){
  tmp_poly = cbind(tmp_poly, I(tmp_poly$year^i))
}
colnames(tmp_poly) = c("year", "temp", paste0("yearpoly", polys))

poly_vars = backward_elimination_temp(tmp_poly, 0.05)
poly_mdl = lm(temp ~ ., data = tmp_poly[,poly_vars])
```

Plot of the polynomial is below. 
```{r}
plot(temp ~ year, data = tmp_poly, main = "Polynomial Fit")
lines(poly_mdl$fitted.values ~ year, data = tmp_poly)

poly_vars
yr = 2020 - min(aatemp$year)
year_2020 = data.frame(yearpoly5 = yr^5, yearpoly7 = yr^7, yearpoly8 = yr^8)
predict(poly_mdl, newdata = year_2020, interval = "prediction")
```
The polynomial model (which uses some very high degree terms after backward elim) predicts a 72.60171 average temp for the year 2020. This is apparent by the right tail of the fit in the plot above which seems like its about to explode upwards (Most likely a byproduct of tail bias). 

##d) 1930 Broken Stick 

Here, we'll use BSR, with the cutoff at 1930. Before 1930, we'll regress with the intercept only $\bar{y}$, and use a SLR with year after:  
```{r}
lhs = function(x){ifelse(x < 1930, 1930 - x, 0)}
rhs = function(x){ifelse(x >= 1930, x - 1930, 0)}
bsr = lm(temp ~ lhs(year) + rhs(year), data = aatemp)
summary(bsr)

#g1 = lm(temp ~ 1, aatemp, subset = (year < 1930) ) 
#g2 = lm(temp ~ year, aatemp, subset = (year >= 1930)) 
#summary(g1)
#summary(g2)
```

The negative coefficient (which is significant at 0.05, but not after using Bonferonni) on the lhs suggests temperatures appear to be decreasing as year increases. This refutes the first claim that temperatures are constant before 1930. The insiginificant coefficent for the rhs suggests that there is no trend after 1930, which refutes the second claim (that there is linear trend after 1930).  

##e) Spline 

Note: 
Spline fit - red
BSR - blue 
AR1 - black 
polynomial - green 

```{r}
tmp_spline = bs(aatemp$year, 6)
temp_spline = lm(aatemp$temp ~ tmp_spline)
plot(temp ~ year, data = aatemp, main = "Splines, AR1, Poly, BSR")
lines(temp_spline$fitted.values ~ year, data = aatemp, col = "red")
lines(bsr$fitted.values ~ year, data = aatemp, col = "blue")
lines(mdl_lag$fitted.values ~ year, data = tmp_lag_clean)
lines(poly_mdl$fitted.values ~ year, data = aatemp, col = "green")
#polynomial backwards 

plot(mdl_raw$residuals)

```

"Does this model (spline) fit better than the straight-line model?"
Yes. The linear model does not work as well here because it's hard to assume that we have a linear model (as suggested by the residual plot of the SLR, the residuals seem to have a reationship with x). The spline fit captures this non linearity, has a SSR (sum sq. residuals), and is also smooth (presumably to prevent overfitting)  